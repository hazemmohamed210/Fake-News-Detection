{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset"
      ],
      "metadata": {
        "id": "-qjGDFchVjUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtD0UGGjVY-P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('WELFake_Dataset.csv')\n",
        "dataset.drop(labels = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "dataset.dropna(axis = 0, inplace = True, ignore_index = True)\n",
        "num_rows, num_cols = dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concatenate title and text column for each article"
      ],
      "metadata": {
        "id": "vuCf_EEPWbno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"content\"] = dataset[\"title\"]+ \" \"+ dataset[\"text\"]"
      ],
      "metadata": {
        "id": "4v6Pt1iqWQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the tf-idf vectorizer"
      ],
      "metadata": {
        "id": "H5SV_UFfWh-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1500)"
      ],
      "metadata": {
        "id": "R3iB1102Wt-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import NLTK tools for text pre-processing"
      ],
      "metadata": {
        "id": "xS3Qs5HiWw_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "BLZ6RR70XAw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to perform pre-processing steps to a chunk of the dataset"
      ],
      "metadata": {
        "id": "50H_6f5YXHW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_chunk(chunk):\n",
        "    processed_chunk = []\n",
        "    for text in chunk:\n",
        "        rev = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        rev = rev.lower()\n",
        "        rev = rev.split()\n",
        "\n",
        "        rev = [ps.stem(word) for word in rev if not word in stop_words]\n",
        "        rev = ' '.join(rev)\n",
        "        processed_chunk.append(rev)\n",
        "    return processed_chunk"
      ],
      "metadata": {
        "id": "dcn8bqSAXYxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset to chunks and pre-process each chunk separately"
      ],
      "metadata": {
        "id": "c2dkVtL4XcQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 1000\n",
        "num_chunks = (num_rows // chunk_size) + 1\n",
        "\n",
        "corpus = []\n",
        "for chunk_num in range(0, num_chunks, 2):\n",
        "    start_idx = chunk_num * chunk_size\n",
        "    end_idx = min((chunk_num + 1) * chunk_size, num_rows)\n",
        "    chunk = dataset['content'][start_idx:end_idx]\n",
        "    pc = preprocess_chunk(chunk)\n",
        "    corpus.extend(pc)\n",
        "\n",
        "    chunk_num+=1\n",
        "\n",
        "    start_idx = chunk_num * chunk_size\n",
        "    end_idx = min((chunk_num + 1) * chunk_size, num_rows)\n",
        "    chunk = dataset['content'][start_idx:end_idx]\n",
        "    pc = preprocess_chunk(chunk)\n",
        "    corpus.extend(pc)"
      ],
      "metadata": {
        "id": "OF_lM4USXrxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform feature extraction on the pre-processed dataset"
      ],
      "metadata": {
        "id": "-peT6T6GX85s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.empty((0,1500))\n",
        "\n",
        "for i in range(0, len(corpus), chunk_size):\n",
        "    corpus_chunk = corpus[i:i+chunk_size]\n",
        "    tfidf_chunk = tfidf.fit_transform(corpus_chunk).toarray()\n",
        "    x = np.concatenate((x, tfidf_chunk), axis=0)"
      ],
      "metadata": {
        "id": "G5ut7ZHqYIZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform train-test split"
      ],
      "metadata": {
        "id": "D_v_Y9tBYc7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = dataset.iloc[:, 2]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "o1DxmjfiYmQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create CNN sequential model"
      ],
      "metadata": {
        "id": "0kaKj6ZhZApn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras import Sequential, utils\n",
        "from keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Input\n",
        "\n",
        "def create_model(filters, kernel_size, pool_size):\n",
        "    keras.backend.clear_session()\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Input(shape=(1500,1)))\n",
        "    classifier.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
        "    classifier.add(MaxPool1D(pool_size=pool_size))\n",
        "    classifier.add(Flatten())\n",
        "    classifier.add(Dense(units=64, activation='relu'))\n",
        "    classifier.add(Dense(units=1, activation='sigmoid'))\n",
        "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(classifier.summary())\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "O7MzD2lNZFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the param grid for the hyper-parameters to be tuned"
      ],
      "metadata": {
        "id": "UyJ2ZxB_ZT8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'filters': [32, 64, 128],\n",
        "    'kernel_size': [5, 7],\n",
        "    \"pool_size\": [2,3],\n",
        "    \"batch_size\": [128, 64]\n",
        "}"
      ],
      "metadata": {
        "id": "QZPm1uSyZcPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the hyper-parameter tuning process"
      ],
      "metadata": {
        "id": "AO8zQB21ZiM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, batch_size=64, epochs=12, filters=32, kernel_size=7, pool_size=2)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=1, error_score='raise', verbose=4)\n",
        "grid_search.fit(X_train_reshaped, y_train)"
      ],
      "metadata": {
        "id": "_GfaUdNbZ2_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "View hyper-parameter tuning results and test the best model on the test set"
      ],
      "metadata": {
        "id": "lkAJx_staAx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_search.cv_results_)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "\n",
        "y_pred = grid_search.predict(X_test_reshaped)"
      ],
      "metadata": {
        "id": "MgrYLHm1aeeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display hyper-parameter tuning results in a latex table"
      ],
      "metadata": {
        "id": "ykYQCeeGcyXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "gridres = pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','rank_test_score']]\n",
        "print(gridres.sort_values(by=['rank_test_score']).to_latex(index=False))"
      ],
      "metadata": {
        "id": "M26qMavhc4_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model performance and plot its confusion matrix"
      ],
      "metadata": {
        "id": "J_8SjQNka92f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, f1_score, recall_score, precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_try)\n",
        "precision = precision_score(y_test, y_try)\n",
        "f1 = f1_score(y_test, y_try)\n",
        "recall = recall_score(y_test, y_try)\n",
        "\n",
        "print(f\"Accuracy = {accuracy:.2f}\\nPrecision = {precision:.2f}\\nF1 Score = {f1:.2f}\\nRecall = {recall:.2f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_try)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=grid_search.classes_)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WOTX6dVUbAYY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}